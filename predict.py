import pandas as pd
import numpy as np
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import log_loss
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Input
from tensorflow.keras.optimizers import Adam

# 1. è¯»å–æ•°æ®
train = pd.read_csv("train.csv")
test = pd.read_csv("test.csv")

# 2. ç‰¹å¾åˆ—
features = ["long", "lat", "rainann", "soildepth", "soilfert",
            "tempann", "topo", "easting", "northing"]

# 3. å‡†å¤‡ä¿å­˜ç»“æœ
all_predictions = []
logscore_list = []

# 4. è·å–æ‰€æœ‰ç‰©ç§
species_list = train["Species"].unique()

# 5. å¯¹æ¯ä¸ªç‰©ç§å»ºæ¨¡
for i, species in enumerate(species_list, start=1):
    print(f"\nğŸ“¦ è®­ç»ƒç‰©ç§ {i}: {species}")

    # æå–è¯¥ç‰©ç§æ•°æ®
    train_s = train[train["Species"] == species].copy()
    test_s = test[test["Species"] == species].copy()
    X = train_s[features].values
    y = train_s["pres.abs"].values

    # pres.abs å…¨ä¸º 0 æˆ– 1 çš„ç‰©ç§è·³è¿‡
    if len(np.unique(y)) < 2:
        print(f"âš ï¸ pres.abs å…¨ä¸º {np.unique(y)[0]}ï¼Œè·³è¿‡")
        continue

    # ç‰¹å¾ç¼©æ”¾
    scaler = MinMaxScaler()
    X_scaled = scaler.fit_transform(X)
    X_test = scaler.transform(test_s[features].values)

    # ç»´åº¦è°ƒæ•´
    X_scaled = X_scaled.reshape((X_scaled.shape[0], X_scaled.shape[1], 1))
    X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))

    # åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†ï¼Œç¡®ä¿åŒ…å« 0 å’Œ 1
    X_train, X_val, y_train, y_val = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y
    )

    # æ„å»º CNN æ¨¡å‹
    model = Sequential([
        Input(shape=(X_train.shape[1], 1)),
        Conv1D(32, kernel_size=3, activation='relu'),
        MaxPooling1D(pool_size=2),
        Flatten(),
        Dense(64, activation='relu'),
        Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer=Adam(), loss='binary_crossentropy', metrics=['accuracy'])

    # è®­ç»ƒæ¨¡å‹
    model.fit(X_train, y_train, epochs=30, batch_size=128, verbose=0)

    # éªŒè¯é¢„æµ‹
    val_pred = model.predict(X_val).flatten()
    val_pred = np.clip(val_pred, 1e-15, 1 - 1e-15)
    logscore = log_loss(y_val, val_pred)
    print(f"âœ… log loss: {logscore:.5f}")
    logscore_list.append((species, logscore))

    # æµ‹è¯•é¢„æµ‹
    test_pred = model.predict(X_test).flatten()
    result_df = pd.DataFrame({
        "id": test_s["id"],
        "pred": test_pred
    })
    all_predictions.append(result_df)

# 6. åˆå¹¶é¢„æµ‹ç»“æœ
submission = pd.concat(all_predictions).sort_values("id")
submission.to_csv("submission.csv", index=False)
print("submission.csv")


